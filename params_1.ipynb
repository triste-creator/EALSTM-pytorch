{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from joblib import dump, load\n",
    "import torch.utils.data as Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        attention_weights = F.softmax(torch.matmul(q, k.transpose(1, 2)) / torch.sqrt(torch.tensor(self.embed_dim)), dim=-1)\n",
    "        output = torch.matmul(attention_weights, v)\n",
    "        # print(\"SelfAttention shape:\"+str(output.shape))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'EALSETM' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 70\u001B[0m\n\u001B[0;32m     68\u001B[0m state \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfinal_model_lstm_att.pt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     69\u001B[0m \u001B[38;5;66;03m# weight =state['ModuleList.weight']\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mstate\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mstate_dict\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mkeys())\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m权重\u001B[39m\u001B[38;5;124m\"\u001B[39m,state)\n",
      "\u001B[1;31mTypeError\u001B[0m: 'EALSETM' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# 定义 EALSETM 模型\n",
    "class EALSETM(nn.Module):\n",
    "    def __init__(self, batch_size, input_dim, hidden_layer_sizes, output_dim, attention_dim):\n",
    "        \"\"\"\n",
    "        预测任务  params:\n",
    "         batch_size       : 批次量大小\n",
    "        input_dim         : 输入数据的维度\n",
    "        hidden_layer_size : 隐层的数目和维度\n",
    "        output_dim        : 输出维度\n",
    "        attention_dim     : 自注意力机制维度\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # 批次量大小\n",
    "        self.batch_size = batch_size\n",
    "        # lstm层数\n",
    "        self.num_layers = len(hidden_layer_sizes)\n",
    "        self.hidden_layer_sizes = len(hidden_layer_sizes)\n",
    "        self.lstm_layers = nn.ModuleList()  # 用于保存LSTM层的列表\n",
    "\n",
    "        # 定义第一层LSTM\n",
    "        self.lstm_layers.append(nn.LSTM(input_dim, hidden_layer_sizes[0], batch_first=True))\n",
    "\n",
    "        # 定义后续的LSTM层\n",
    "        for i in range(1, self.num_layers):\n",
    "                self.lstm_layers.append(nn.LSTM(hidden_layer_sizes[i-1], hidden_layer_sizes[i], batch_first=True))\n",
    "\n",
    "        # 定义自注意力层\n",
    "        self.attention = SelfAttention(attention_dim)\n",
    "\n",
    "         # 序列平均池化操作\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "\n",
    "        # 定义线性层\n",
    "        self.linear  = nn.Linear(hidden_layer_sizes[-1], output_dim)\n",
    "\n",
    "\n",
    "    def forward(self, input_seq):  # torch.Size([16, 512])\n",
    "        # 前向传播的过程是输入->LSTM层->全连接层->输出\n",
    "        # 在观察查看LSTM输入的维度，LSTM的第一个输入input_size维度是(batch, seq_length, dim) batch是batch size , seq_length是序列长度，dim是输入维度，也就是变量个数\n",
    "        # LSTM的第二个输入是一个元组，包含了h0,c0两个元素，这两个元素的维度都是（D∗num_layers,N,out)，\n",
    "        # D=1表示单向网络，num_layers表示多少个LSTM层叠加，N是batch size，out表示隐层神经元个数\n",
    "\n",
    "        # 数据预处理\n",
    "        #改变输入形状，适应网络输入[batch, seq_length, dim]\n",
    "        lstm_out = input_seq\n",
    "        # 获取隐藏层数据\n",
    "        hidden_states = []\n",
    "        for lstm in self.lstm_layers:\n",
    "            lstm_out, hidden= lstm(lstm_out)  ## 进行一次LSTM层的前向传播\n",
    "            hidden_states += hidden\n",
    "        # print(\"lstm_out.size\"+str(lstm_out.size()))  # torch.Size([64, 7, 64])\n",
    "\n",
    "        # 送入注意力机制层\n",
    "        attention_features = self.attention(lstm_out)  # torch.Size([64, 7, 64])\n",
    "\n",
    "         # 自适应平均池化\n",
    "        x = self.adaptive_pool(attention_features.permute(0,2,1))  # torch.Size([64, 64, 1])\n",
    "        # 平铺\n",
    "        x = x.view(self.batch_size, -1)  # 把数据平铺\n",
    "        # 全连接层\n",
    "        predict = self.linear(x) # torch.Size([64, 10]\n",
    "        return predict\n",
    "\n",
    "\n",
    "state = torch.load(\"final_model_lstm_att.pt\")\n",
    "# weight =state['ModuleList.weight']\n",
    "print(state['state_dict'].keys())\n",
    "print(\"权重\",state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}